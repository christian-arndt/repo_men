{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, RocCurveDisplay, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score',\n",
    "          'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the ROC Curve\n",
    "\n",
    "The **Receiver Operating Characteristic (ROC)** curve is a graphical representation used to evaluate the performance of a binary classification model. It plots the **True Positive Rate (TPR)** against the **False Positive Rate (FPR)** at various threshold settings. Below is a an illustration of the ROC:\n",
    "\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/1/13/Roc_curve.svg\" alt=\"ROC Curve\" width=\"400\"/>\n",
    "\n",
    "\n",
    "- **True Positive Rate (TPR)**: Also known as sensitivity or recall, it measures the proportion of actual positives correctly identified.\n",
    "\n",
    "$$\n",
    "  TPR = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "$$\n",
    "\n",
    "- **False Positive Rate (FPR)**: It measures the proportion of actual negatives that were incorrectly classified as positive.\n",
    "$$\n",
    "  FPR = \\frac{\\text{False Positives}}{\\text{False Positives} + \\text{True Negatives}}\n",
    "$$\n",
    "\n",
    "\n",
    "### Interpretation\n",
    "- A model with an AUC close to 1.0 is highly capable of distinguishing between classes.\n",
    "- A model with an AUC near 0.5 performs no better than random chance.\n",
    "\n",
    "The ROC curve is useful for visualizing the trade-off between sensitivity and specificity across different thresholds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = RocCurveDisplay.from_predictions(\n",
    "    ### Your code here\n",
    "    color=\"#0000AA\",\n",
    "    plot_chance_level=True,\n",
    ")\n",
    "\n",
    "_ = display.ax_.set(\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Confusion Matrix\n",
    "\n",
    "The **Confusion Matrix** is a table that summarizes the performance of a classification model by comparing the predicted labels with the actual labels. It provides a detailed breakdown of correct and incorrect classifications for each class.\n",
    "\n",
    "### Structure of a Confusion Matrix:\n",
    "|                | **Predicted: Positive** | **Predicted: Negative** |\n",
    "|----------------|--------------------------|--------------------------|\n",
    "| **Actual: Positive** | True Positive (TP)        | False Negative (FN)        |\n",
    "| **Actual: Negative** | False Positive (FP)       | True Negative (TN)         |\n",
    "\n",
    "### Memory items:\n",
    "1. **True Positive (TP)**: Cases where the model correctly predicts the positive class.\n",
    "2. **False Negative (FN)**: Cases where the model fails to predict the positive class (misses a positive case).\n",
    "3. **False Positive (FP)**: Cases where the model incorrectly predicts the positive class (false alarm).\n",
    "4. **True Negative (TN)**: Cases where the model correctly predicts the negative class.\n",
    "\n",
    "### Some Metrics Derived from the Confusion Matrix:\n",
    "- **Accuracy**: The overall correctness of the model.\n",
    "$$\n",
    "  \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "- **Precision**: The proportion of positive predictions that are actually correct.\n",
    "$$\n",
    "  \\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "- **Recall (Sensitivity)**: The proportion of actual positives that are correctly identified.\n",
    "$$\n",
    "  \\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "- **F1-Score**: The harmonic mean of precision and recall.\n",
    "$$\n",
    "  \\text{F1-Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the \"Clinical Score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going Deep With Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "behavioral-data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
