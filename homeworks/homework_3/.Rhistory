library(tidyverse)
library(ggcorrplot)
require(psych)
library(lavaan)
library(lavaanPlot)
setwd('/Users/chris/Documents/Behavioral Data Science/repository/repo_men/chris_only')
setwd('/Users/chris/Documents/Behavioral Data Science/repository/repo_men/homeworks/homework_3')
# Read in data
data = read.table("data/personality_data.csv", header = T , sep= ";")
# Get only A subscale data ("Warmth")
data = data %>% select(matches("^ABCDE"),  -c("age", "accuracy", "country"))
View(data)
# Get only A subscale data ("Warmth")
data = data %>% select(matches("^[ABCDE]"),  -c("age", "accuracy", "country"))
# Read in data
data = read.table("data/personality_data.csv", header = T , sep= ";")
# Get only A subscale data ("Warmth")
data = data %>% select(matches("^[ABCDE]"),  -c("age", "accuracy", "country"))
View(data)
# Read in data
data = read.table("data/personality_data.csv", header = T , sep= ";")
# Get only A subscale data ("Warmth")
data_selections = data %>% select(matches("^[ABCDE]"),  -c("age", "accuracy", "country", "elapsed"))
## Next, we remove participants with suspicious elapsed time
median_elapsed = median(data$elapsed, na.rm = T)
mad_elapsed = median(abs(data$elapsed - median_elapsed), na.rm = T)
mask = data$elapsed < median_elapsed + 3*mad_elapsed &
data$elapsed > median_elapsed - 3*mad_elapsed
data_ABC = data_ABC[mask,]
data_selections = data_selections[mask,]
# Read in data
data = read.table("data/personality_data.csv", header = T , sep= ";")
# Get only ABCDE subscale data
data_selections = data %>% select(matches("^[ABCDE]"),  -c("age", "accuracy", "country", "elapsed"))
## Next, we remove participants with suspicious elapsed time
median_elapsed = median(data$elapsed, na.rm = T)
mad_elapsed = median(abs(data$elapsed - median_elapsed), na.rm = T)
mask = data$elapsed < median_elapsed + 3*mad_elapsed &
data$elapsed > median_elapsed - 3*mad_elapsed
data_selections = data_selections[mask,]
mask = data$elapsed < median_elapsed + 1.5*mad_elapsed &
data$elapsed > median_elapsed - 3*mad_elapsed
data_selections = data_selections[mask,]
View(data_selections)
# Ensure no missings present
sum(data_selections == 0)
# Ensure no missings present
data_selections[data_selections == 0] = NA
sum(is.na(data_selections))
sum(!is.na(data_selections))
# Read in data
data = read.table("data/personality_data.csv", header = T , sep= ";")
# Get only ABCDE subscale data
data_selections = data %>% select(matches("^[ABCDE]"),  -c("age", "accuracy", "country", "elapsed"))
## Next, we remove participants with suspicious elapsed time
median_elapsed = median(data$elapsed, na.rm = T)
mad_elapsed = median(abs(data$elapsed - median_elapsed), na.rm = T)
mask = data$elapsed < median_elapsed + 1.5*mad_elapsed &
data$elapsed > median_elapsed - 3*mad_elapsed
data_selections = data_selections[mask,]
sum(is.na(data_selections))
# Ensure no missings present
data_selections[data_selections == 0] = NA
# Impute NAs
for(i in 1:ncol(data_selections)){
data_selections[is.na(data_selections[,i]), i] <- mean(data_selections[,i], na.rm = TRUE)
}
# Ensure imputation worked
sum(is.na(data_selections))
# Generate and inspect heatmap of correlation matrix
corr = cor(data_selections)
ggcorrplot(corr, lab = T)
ggcorrplot(corr, lab = F)
screeplot(data_selections)
library(ggplot2)
# compute pca
pca <- prcomp(data_selections)
# compute total variance
variance = pca $sdev^2 / sum(pca $sdev^2)
# Scree plot
qplot(c(1:4), variance) +
geom_line() +
geom_point(size=4)+
xlab("Principal Component") +
ylab("Variance Explained") +
ggtitle("Scree Plot") +
ylim(0, 1)
# Scree plot
qplot(c(1:4), variance) +
geom_line() +
xlab("Principal Component") +
ylab("Variance Explained") +
ggtitle("Scree Plot") +
ylim(0, 1)
# Extract the eigenvalues from the PCA object
eigenvalues <- pca$sdev^2
# Create a scree plot
plot(eigenvalues, type = "b",
xlab = "Principal Component",
ylab = "Eigenvalue")
# Add a line at y = 1 to indicate the elbow
abline(v = 2, col = "red")
library(tidyverse)
library(ggcorrplot)
require(psych)
library(lavaan)
library(lavaanPlot)
library(ggplot2)
setwd('/Users/chris/Documents/Behavioral Data Science/repository/repo_men/homeworks/homework_3')
# Read in data
data = read.table("data/personality_data.csv", header = T , sep= ";")
# Get only ABCDE subscale data
data_selections = data %>% select(matches("^[ABCDE]"),  -c("age", "accuracy", "country", "elapsed"))
## Next, we remove participants with suspicious elapsed time
median_elapsed = median(data$elapsed, na.rm = T)
mad_elapsed = median(abs(data$elapsed - median_elapsed), na.rm = T)
mask = data$elapsed < median_elapsed + 1.5*mad_elapsed &
data$elapsed > median_elapsed - 3*mad_elapsed
data_selections = data_selections[mask,]
# Ensure no missings present
# 0s are equivalent to NA values for these columns
data_selections[data_selections == 0] = NA
# Impute NAs
for (i in 1:ncol(data_selections)) {
data_selections[is.na(data_selections[,i]), i] <- mean(data_selections[,i], na.rm = TRUE)
}
# Ensure imputation worked
sum(is.na(data_selections))
# Generate and inspect heatmap of correlation matrix
corr = cor(data_selections)
ggcorrplot(corr, lab = F)
# compute pca
pca <- prcomp(data_selections)
# compute total variance
variance = pca $sdev^2 / sum(pca $sdev^2)
# Extract the eigenvalues from the PCA object
eigenvalues <- pca$sdev^2
# Create a scree plot
plot(eigenvalues, type = "b",
xlab = "Principal Component",
ylab = "Eigenvalue")
efa.model <- '
efa("efa")*f1 +
efa("efa")*f2 +
efa("efa")*f3 +
efa("efa")*f4 +
efa("efa")*f5
'
fit <- cfa(efa.model, data = HolzingerSwineford1939)
fit <- factanal(data_selections, 5)
summary(fit, standardized = TRUE)
summary(fit)
View(fit)
# Now, we need to report the loadings and comput and report the communalities
print(fit$loadings)
# Now, we need to report the loadings and comput and report the communalities
print(loadings(fit, digits=3, cutoff=.01))
# Now, we need to report the loadings and comput and report the communalities
print(loadings(fit, digits=3, cutoff=0))
# Now, we need to report the loadings and comput and report the communalities
print(loadings(fit), digits=3, cutoff=0)
# Now, we need to report the loadings and comput and report the communalities
print(loadings(fit), digits=3, cutoff=0)
# Now, we need to report the loadings and comput and report the communalities
loadings = data.frame(loadings(fit))
View(loadings)
# Now, we need to report the loadings and comput and report the communalities
loadings = data.frame(loadings(fit), c=["F1", "F2", "F3", "F4", "F5"])
# Now, we need to report the loadings and comput and report the communalities
loadings = data.frame(loadings(fit), col=["F1", "F2", "F3", "F4", "F5"])
# Now, we need to report the loadings and comput and report the communalities
loadings = data.frame(loadings(fit), name=["F1", "F2", "F3", "F4", "F5"])
# Now, we need to report the loadings and comput and report the communalities
loadings = data.frame(loadings(fit), name=c("F1", "F2", "F3", "F4", "F5"))
View(loadings)
# Now, we need to report the loadings and comput and report the communalities
loadings = data.frame(loadings(fit), col=c("F1", "F2", "F3", "F4", "F5"))
View(loadings)
loadings.columns = c("F1", "F2", "F3", "F4", "F5")
View(loadings)
# Now, we need to report the loadings and comput and report the communalities
loadings = data.frame(loadings(fit))
View(loadings)
colnames(loadings) = c("F1", "F2", "F3", "F4", "F5")
colnames(loadings) = c("F1", "F2", "F3", "F4", "F5")
View(loadings)
library(gridExtra)
pdf("data_output.pdf", height=11, width=8.5)
grid.table(loadings)
knitr::kable(loadings, format = "html")
install.packages("knitr")
library(knitr)
# Create a nice-looking table for HTML or Markdown
kable(loadings, caption = "Factor Loadings from Factor Analysis")
# Extract the factor loadings
loadings <- fit$loadings
# Convert the loadings to a data frame
loadings_df <- as.data.frame(loadings)
# Set appropriate column names for the factors (F1, F2, ..., F5)
colnames(loadings_df) <- c("F1", "F2", "F3", "F4", "F5")
# Create a nice-looking table for HTML or Markdown
library(knitr)
kable(loadings_df, caption = "Factor Loadings from Factor Analysis")
dim(loadings)
print(loadings(fit), digits=3, cutoff=0)
# Extract the factor loadings
loadings_df <- as.data.frame(fit$loadings)
# Set appropriate column names (F1, F2, F3, F4, F5)
colnames(loadings_df) <- c("F1", "F2", "F3", "F4", "F5")
dim(loadings_df)
# Extract the factor loadings
loadings_df <- as.data.frame(unclass(fit$loadings))
dim(loadings_df)
# Set appropriate column names (F1, F2, F3, F4, F5)
colnames(loadings_df) <- c("F1", "F2", "F3", "F4", "F5")
kable(loadings_df)
xtable(loadings_df)
library(xtable)
xtable(loadings_df)
# Extract communalities
communalities_df = rowSums(loadings_df ** 2)
print(loadings_df[1])
print(loadings_df[:,1])
print(loadings_df[,1])
check = print(loadings_df[,1])
check = check ** 2
check = loadings_df[,1]
print(check)
print(check)
check = check ^ 2
print(check)
# Extract communalities
communalities_df = rowSums(loadings_df ^ 2)
# Extract communalities
communalities_df = rowSums(loadings_df ** 2)
# Extract communalities
communalities_df = rowSums(loadings_df ^ 2)
# Extract communalities
communalities = rowSums(loadings_df ^ 2)
dim(communalities)
print(communalities)
xtable(communalities)
print(rowSums(loadings_df))
# Extract communalities
communalities = rowSums(loadings_df ^ 2)
# Extract communalities
communalities = as.data.frame(rowSums(loadings_df ^ 2))
View(communalities)
# Extract communalities
communalities_df = as.data.frame(rowSums(loadings_df ^ 2))
xtable(communalities_df)
